# There are four general steps through which data flows within an organization.

| Collection & Ingestion     | Store & Management    | Process   |
| :------------- | :----------: | -----------: |
|  Amazon Web Services| Amazon S3   | Hadoop   |
| Google Cloud   | Hadoop| Hive  |
| Azure | Amazon Reshift | Apache Spark |

Data is stored in raw format. The next step is to prepare it, which includes "cleaning data", for instance finding missing or duplicate values, and converting data into a more organized format.

Once the data is clean and organized, it can be exploited. We explore it, visualize it, build dashboards to track changes or compare two sets of data.

Finally, once we have a good grasp of our data, we're ready to run experiments, like evaluate which article title gets the most hits, or to build predictive models, for example to forecast stock prices.

Data engineers are responsible for the first step of the process: ingesting collected data and storing it. They have a great responsibility as they lay the ground work for data analysts, data scientists and machine learning engineers. If the data is scattered around, corrupted, and difficult to access, there's not much to prepare, explore, or experiment with.

Data engineer: their job is to deliver the correct data, in the right form, to the right people, as efficiently as possible.

A data engineer's responsibilities
They ingest data from different sources, optimize the databases for analysis, and manage data corruption. Data engineers develop, construct, test, and maintain architectures such as databases and large-scale processing systems to process and handle massive amounts of data. If you're not sure what this all means, that's okay! The course will unpack all this jargon and explain the what, why, and how.

With the advent of big data,the demand for data engineers has increased. Big data can be defined as data so large you have to think about how to deal with its size, because it's difficult to process using traditional data management methods.

## The five "V"s
Big data is commonly characterized by five Vs:
volume (the quantity of data points),
variety (type and nature of the data: text, image, video, audio),
velocity (how fast the data is generated and processed),
veracity (how trustworthy the sources are),
value (how actionable the data is).
Data engineers need to take all of this into consideration.

# The Data Pipeline - Data is the new oil
Ingest
Process
Store

Need Pipelines
Automate flow from one station to the next
Provide up-to-date, accurate, relavant data

Data Pipelines Ensure Data Pipeline
Extracting
Transforming
Combining
Validating
Loading

RESPONSIBILITIES
• Design and develop Data Warehouse and Data Mart solutions to support business reporting requirements of different stakeholders across multiple business verticals including Finance, Marketing, Logistics, Product etc.
• Conceptualize, design, and development of data models, entities, and relationships
• Work with the Data Science team to understand data structure and sourcing needs to enable them to build use cases
• Adhere to processes and design best practices to ensure data extractions meet quality standards and are enhanced for analytical purposes to support “single source of truth”
• Develop the vision and map strategy to provide proactive solutions and enable stakeholders to extract insights and value from data
• Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions
• Develop ELT solution to load data to/from the Operational Data Store (ODS), Data Warehouse, and Datamart using DBT
• Work with data engineers to troubleshoot application problems and effectively resolve issues
• Formalize metadata descriptions and capture methods and workflows

QUALIFICATIONS
• Knowledge of data warehouse concepts and implementation
• Experience developing data models in a data warehouse environment; experience with big data architectures
• Experience working with ETL/ELT concepts of data integration, consolidation, enrichment, and aggregation
- Extracting,Transforming,Loading
- ELT stands for "Extract, Load, and Transform." In this process, data gets leveraged via a data warehouse in order to do basic transformations. That means there's no need for data staging. ELT uses cloud-based data warehousing solutions for all different types of data - including structured, unstructured, semi-structured, and even raw data types.
-
• Strong understanding of dimensional modeling and similar data warehousing techniques
• Experience working with relational or multidimensional databases and business intelligence architectures, such as PostgreSQL and Redshift

• Strong understanding of SQL and working knowledge of using SQL (prefer PostgreSQL and Redshift) for various reporting and transformation needs
• Familiar with at least one of the programming languages: Python, Java

• 2+ years in building a data warehouse and data pipelines or 3+ years in data intensive engineering role
• Experience running Agile methodology and applying Agile to data engineering
• Bachelor’s degree or equivalent required
• Familiarity with AWS ecosystem, including RDS, Redshift, Glue, Athena, etc. a plus
• Experiences with Apache Hadoop, Hive, Spark and PySpark a plus
• Familiarity with DBT and AirFlow a plus
• Familiarity with REST for accessing cloud based services a plus
